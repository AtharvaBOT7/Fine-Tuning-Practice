{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c66983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.quantization as tq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abcd6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005f4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23010c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_config = tq.QConfig(\n",
    "    activation = tq.FakeQuantize.with_args(\n",
    "        observer = tq.MovingAverageMinMaxObserver,\n",
    "        quant_min = 0, quant_max = 255,\n",
    "        dtype = torch.quint8, \n",
    "        qscheme = torch.per_tensor_affine\n",
    "    ),\n",
    "    weight = tq.FakeQuantize.with_args(\n",
    "        observer = tq.MinMaxObserver,\n",
    "        quant_min = -128, quant_max = 127,\n",
    "        dtype = torch.qint8,\n",
    "        qscheme = torch.per_tensor_symmetric\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55afa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to quantize only the linear layers and not the embedding layers\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        m.qconfig = None\n",
    "    if isinstance(m, nn.LayerNorm):\n",
    "        m.qconfig = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ef2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.qconfig = qat_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c338261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/v95prvpj4cz7y53gvmgm1d_r0000gn/T/ipykernel_40735/1317648221.py:4: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  tq.prepare_qat(model, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(\n",
       "    in_features=768, out_features=50257, bias=False\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will now fuse the model with fake quantization operations\n",
    "\n",
    "model.train()\n",
    "tq.prepare_qat(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5da15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fine tuning the quantization aware model \n",
    "\n",
    "inputs = tokenizer(\"Quantization Aware Training in LLMs!\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7614b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a99f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a46882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 9.39015007019043\n",
      "Step 10, Loss: 2.0682854652404785\n",
      "Step 20, Loss: 0.5169497132301331\n",
      "Step 30, Loss: 0.006866047624498606\n",
      "Step 40, Loss: 0.013810182921588421\n"
     ]
    }
   ],
   "source": [
    "for step in range(50):\n",
    "    ouptuts = model(**inputs, labels=labels)\n",
    "    loss = ouptuts.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c375599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/v95prvpj4cz7y53gvmgm1d_r0000gn/T/ipykernel_40735/3275049686.py:3: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  qat_model = tq.convert(model.eval(), inplace=False)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::linear_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Converting the model to a fully quantized model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m qat_model = \u001b[43mtq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/typing_extensions.py:3004\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3001\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(arg)\n\u001b[32m   3002\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   3003\u001b[39m     warnings.warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3004\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:664\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[32m    663\u001b[39m     module = copy.deepcopy(module)\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[32m    673\u001b[39m     _remove_qconfig(module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:729\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    718\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[32m    719\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[32m    720\u001b[39m     ):\n\u001b[32m    721\u001b[39m         _convert(\n\u001b[32m    722\u001b[39m             mod,\n\u001b[32m    723\u001b[39m             mapping,\n\u001b[32m   (...)\u001b[39m\u001b[32m    727\u001b[39m             use_precomputed_fake_quant=use_precomputed_fake_quant,\n\u001b[32m    728\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m     reassign[name] = \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n\u001b[32m    734\u001b[39m     module._modules[key] = value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:771\u001b[39m, in \u001b[36mswap_module\u001b[39m\u001b[34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    769\u001b[39m sig = inspect.signature(qmod.from_float)\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_precomputed_fake_quant\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig.parameters:\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m     new_mod = \u001b[43mqmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    775\u001b[39m     new_mod = qmod.from_float(mod)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:341\u001b[39m, in \u001b[36mLinear.from_float\u001b[39m\u001b[34m(cls, mod, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m dtype == torch.qint8, \u001b[33m\"\u001b[39m\u001b[33mWeight observer must have dtype torch.qint8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m qweight = _quantize_weight(mod.weight.float(), weight_post_process)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m qlinear = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m qlinear.set_weight_bias(qweight, mod.bias)\n\u001b[32m    343\u001b[39m qlinear.scale = \u001b[38;5;28mfloat\u001b[39m(act_scale)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:173\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias_, dtype)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsupported dtype specified for quantized Linear!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28mself\u001b[39m._packed_params = \u001b[43mLinearPackedParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28mself\u001b[39m._packed_params.set_weight_bias(qweight, bias)\n\u001b[32m    175\u001b[39m \u001b[38;5;28mself\u001b[39m.scale = \u001b[32m1.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:31\u001b[39m, in \u001b[36mLinearPackedParams.__init__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype == torch.float16:\n\u001b[32m     30\u001b[39m     wq = torch.zeros([\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m], dtype=torch.float)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:38\u001b[39m, in \u001b[36mLinearPackedParams.set_weight_bias\u001b[39m\u001b[34m(self, weight, bias)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;129m@torch\u001b[39m.jit.export\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_weight_bias\u001b[39m(\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mself\u001b[39m, weight: torch.Tensor, bias: Optional[torch.Tensor]\n\u001b[32m     36\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype == torch.qint8:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28mself\u001b[39m._packed_params = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear_prepack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype == torch.float16:\n\u001b[32m     40\u001b[39m         \u001b[38;5;28mself\u001b[39m._packed_params = torch.ops.quantized.linear_prepack_fp16(weight, bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/_ops.py:1255\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Didn't find engine for operation quantized::linear_prepack NoQEngine"
     ]
    }
   ],
   "source": [
    "# Converting the model to a fully quantized model\n",
    "\n",
    "qat_model = tq.convert(model.eval(), inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6c462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
