{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mwTXpnY5WrM-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "Q-fUCKQcWuqW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCXE9ZwdWutC",
        "outputId": "1f49a9e2-f6a8-46f8-91a6-aa8a36bf03df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 38.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.07MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.82MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TeacherMLP(nn.Module):\n",
        "  def __init__(self, hidden1=512, hidden2=256):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(28*28, hidden1),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden1,hidden2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden2,10)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "Ozp-IlQnWuvl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = TeacherMLP(hidden1=512, hidden2=256)"
      ],
      "metadata": {
        "id": "y8DbNVoDWuxt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQW0YpALWu0G",
        "outputId": "738be721-eb56-4fc7-a9d2-cef068728c3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TeacherMLP(\n",
              "  (net): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_teacher(model, loader, epochs=1, lr=1e-3):\n",
        "\n",
        "  opt = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for ep in range(epochs):\n",
        "    total_loss = 0\n",
        "    for x,y in loader:\n",
        "      opt.zero_grad()\n",
        "      out = model(x)\n",
        "      loss = loss_fn(out,y)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      total_loss += loss.item()\n",
        "  print(f\"Teacher Epoch: {ep} loss {total_loss}\")"
      ],
      "metadata": {
        "id": "cwdjkkdqWu20"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_teacher(teacher, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVMb_iXHWu5c",
        "outputId": "f816769d-e283-44d8-a99f-e87ee1a044bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Epoch: 0 loss 282.7695094048977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(teacher.state_dict()) # these are the updated weights of the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB-NfsfnWu79",
        "outputId": "b9222849-26c4-4919-b198-e553a6b01a56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict({'net.1.weight': tensor([[ 0.0075, -0.0234,  0.0022,  ...,  0.0235,  0.0270,  0.0070],\n",
            "        [ 0.0235,  0.0189, -0.0247,  ..., -0.0149,  0.0038,  0.0261],\n",
            "        [-0.0226,  0.0030, -0.0190,  ..., -0.0184,  0.0064, -0.0256],\n",
            "        ...,\n",
            "        [-0.0169, -0.0235,  0.0375,  ..., -0.0139,  0.0089,  0.0351],\n",
            "        [ 0.0107,  0.0285,  0.0291,  ...,  0.0138,  0.0184,  0.0191],\n",
            "        [-0.0241,  0.0211, -0.0102,  ..., -0.0309,  0.0133,  0.0228]]), 'net.1.bias': tensor([ 1.0554e-02, -3.8497e-02, -1.2759e-02, -8.4531e-03, -3.0692e-02,\n",
            "         1.5226e-02,  2.4943e-02, -1.7480e-02,  3.0454e-02,  1.7203e-02,\n",
            "        -3.8991e-02,  2.1866e-02, -8.2824e-04, -2.2781e-02, -3.7418e-02,\n",
            "        -3.0662e-02,  1.2659e-02, -2.2236e-03,  8.2246e-03, -1.7084e-02,\n",
            "         1.4090e-02, -2.5376e-02, -3.8082e-02,  1.1131e-02, -1.9621e-02,\n",
            "        -1.8718e-02, -1.8419e-02, -3.1126e-02, -1.5475e-02, -1.4378e-02,\n",
            "        -2.5952e-02, -2.9957e-02, -3.0219e-02, -2.4979e-02, -3.0865e-02,\n",
            "        -3.8802e-02, -3.4665e-02, -2.7501e-02,  2.9876e-02, -1.1691e-02,\n",
            "        -4.6323e-02, -3.2255e-02, -8.4931e-03, -3.7376e-02,  1.6423e-02,\n",
            "         1.0896e-02,  1.4556e-02, -4.9107e-03,  3.0135e-02, -1.4790e-02,\n",
            "         1.0879e-02, -3.2403e-02,  2.7868e-02, -3.3295e-02,  1.6889e-02,\n",
            "        -1.0917e-03, -1.6484e-02, -2.3549e-02, -2.4771e-02,  2.8850e-02,\n",
            "         1.8529e-03,  8.0558e-04, -9.6946e-03, -3.5307e-02, -3.3420e-02,\n",
            "        -2.2363e-02,  2.8739e-03, -2.9629e-03, -1.4530e-03,  2.6245e-02,\n",
            "         3.4500e-02,  1.4231e-02,  2.0775e-02,  1.9533e-02, -3.7659e-02,\n",
            "        -2.4387e-02, -2.5159e-02,  9.7697e-03, -2.8708e-02,  2.0103e-02,\n",
            "        -8.5721e-03, -4.0382e-02, -1.3363e-02,  9.1748e-03,  1.1577e-02,\n",
            "         1.7833e-02,  2.8636e-02,  2.1472e-02, -4.0698e-02, -1.4747e-03,\n",
            "        -1.3898e-02, -1.8376e-02, -3.5855e-03,  1.0870e-03, -2.6487e-02,\n",
            "        -3.5774e-03,  1.0531e-02, -2.7535e-02, -2.7681e-02, -2.4324e-02,\n",
            "         2.7332e-02, -2.3189e-02,  1.3478e-02, -3.9427e-02, -3.6472e-02,\n",
            "        -1.2419e-02, -6.2918e-03, -9.1360e-03, -7.3069e-03,  3.1779e-02,\n",
            "        -9.9235e-03,  1.0086e-03, -1.0008e-02, -1.4327e-02,  1.9631e-02,\n",
            "         1.1566e-03, -2.6490e-02, -3.3706e-02, -6.8310e-03, -1.9011e-02,\n",
            "        -1.3375e-02, -1.0801e-02, -1.4069e-02, -3.5357e-02,  1.0856e-02,\n",
            "        -8.9801e-03,  3.9540e-03,  2.0885e-02, -2.9902e-02, -3.0761e-02,\n",
            "        -1.0923e-02, -4.1025e-02, -3.1164e-02,  4.1998e-03,  1.9004e-02,\n",
            "         1.2695e-02, -2.3504e-02,  8.3905e-03, -2.1665e-02, -3.0904e-02,\n",
            "        -3.3926e-02, -3.3799e-02, -9.6351e-03,  9.5784e-03, -3.1384e-02,\n",
            "        -5.0686e-03,  1.0165e-02, -2.1362e-02, -5.5729e-03, -4.9026e-04,\n",
            "        -8.0478e-03, -2.6719e-02, -1.7765e-02,  8.4326e-03,  1.1318e-02,\n",
            "        -2.3188e-02, -1.1887e-02, -2.6239e-02, -1.5392e-02, -3.5215e-02,\n",
            "         2.7330e-02,  1.3406e-02, -9.8528e-03, -1.2397e-04, -7.6111e-03,\n",
            "         3.8633e-03, -2.9200e-02, -1.3051e-02,  3.0030e-02, -3.9081e-02,\n",
            "        -2.2208e-02,  2.9214e-02, -2.5319e-02, -1.1372e-02,  1.8806e-02,\n",
            "        -4.7556e-03, -3.0685e-02,  2.9055e-02,  2.6706e-03, -3.4839e-02,\n",
            "        -3.3869e-02, -2.1272e-02,  2.8754e-02, -2.4291e-02,  1.7561e-02,\n",
            "         1.9481e-03, -1.3560e-02,  2.5691e-02, -1.6603e-02, -1.4442e-02,\n",
            "         3.0215e-02, -2.3540e-02,  3.4553e-02, -1.3150e-02, -1.2008e-02,\n",
            "        -1.4523e-03,  2.9198e-02, -6.2992e-03, -2.2607e-02,  3.7004e-03,\n",
            "        -3.9381e-02,  5.3814e-03, -1.5175e-02,  2.6860e-02, -1.5778e-03,\n",
            "        -2.2277e-02, -4.8402e-04, -9.5000e-03,  1.1566e-02,  3.4934e-02,\n",
            "         2.2691e-02, -4.0292e-02, -8.9236e-03,  2.3227e-02,  1.2335e-02,\n",
            "        -1.5846e-04,  2.4107e-02, -3.8562e-02, -3.0489e-02, -5.7149e-03,\n",
            "        -1.4435e-02, -3.2750e-02,  5.4260e-03, -1.0497e-02, -4.8919e-04,\n",
            "         2.4037e-02, -1.3805e-02, -8.5785e-04,  7.9253e-03, -2.1868e-02,\n",
            "        -8.6322e-03, -6.2270e-03, -2.5977e-02, -1.9221e-02,  5.1037e-03,\n",
            "         1.7908e-02,  2.0505e-03,  1.3011e-02, -8.2646e-03,  1.1754e-02,\n",
            "         1.7709e-02,  2.8392e-02, -1.9451e-02, -2.4982e-02,  2.3676e-03,\n",
            "         2.5025e-02, -3.1870e-02,  2.4830e-02,  2.4958e-02, -1.4578e-02,\n",
            "        -8.6736e-03, -2.1087e-02, -1.4522e-02,  1.4881e-02,  5.7944e-03,\n",
            "         9.8480e-04,  5.0904e-03,  2.7196e-02,  1.4244e-02,  2.5832e-02,\n",
            "        -2.8319e-02, -1.1316e-02,  5.9667e-03, -3.1220e-02, -1.7874e-02,\n",
            "         2.9093e-02, -3.7630e-02, -4.0021e-02,  1.6678e-02,  2.7292e-02,\n",
            "        -3.8787e-02, -3.9528e-02,  1.9982e-02,  3.3358e-02,  2.9759e-02,\n",
            "        -5.0211e-03, -7.0769e-03, -3.0771e-03, -3.3399e-02,  1.7222e-02,\n",
            "        -1.1500e-02,  3.4232e-02, -6.4817e-03, -3.8668e-02, -3.0528e-02,\n",
            "        -1.0768e-02, -1.0472e-02,  1.0164e-04,  2.3196e-03, -3.5554e-02,\n",
            "        -3.1519e-02,  1.8880e-02,  3.4098e-02,  7.6609e-03,  3.0359e-02,\n",
            "        -1.7390e-02,  1.1282e-03, -2.5385e-02,  1.4388e-04,  2.8299e-02,\n",
            "         1.4744e-02, -2.7737e-04, -1.5260e-02,  1.8177e-02, -2.4387e-02,\n",
            "        -4.1024e-02,  1.1884e-02,  1.8865e-02, -1.2689e-02,  1.7910e-02,\n",
            "        -3.2240e-02, -3.9016e-02, -8.7984e-03,  2.1439e-02, -3.7543e-03,\n",
            "        -8.4197e-03,  2.1261e-02,  7.1692e-03, -3.2761e-02,  9.2531e-03,\n",
            "         2.4933e-03, -3.0458e-02, -1.3738e-02,  7.2900e-03,  2.2453e-02,\n",
            "         2.5818e-02,  1.3415e-04, -1.5457e-02, -3.2902e-02,  1.4550e-02,\n",
            "        -7.4342e-03,  2.9584e-02,  2.2636e-02, -2.7423e-02, -2.6438e-02,\n",
            "        -2.2499e-03, -2.0375e-02, -3.9676e-02, -3.5737e-02, -7.6471e-03,\n",
            "        -1.9797e-02,  6.3631e-03,  2.6052e-02,  1.9755e-02,  1.4065e-02,\n",
            "        -3.3073e-02, -8.0663e-03, -3.2920e-02, -2.6735e-02, -2.7889e-02,\n",
            "        -4.4090e-02, -4.1239e-02,  1.8153e-02,  1.8003e-02,  1.5472e-02,\n",
            "         2.5878e-02,  1.5239e-02,  1.3526e-02,  9.0943e-03,  7.9976e-04,\n",
            "         3.0616e-02, -2.9204e-02, -1.4726e-02,  3.0513e-02, -2.4112e-02,\n",
            "        -6.8620e-03, -2.7255e-02, -2.9107e-03,  6.0686e-03,  3.7518e-02,\n",
            "        -2.1749e-02,  2.1750e-02,  8.0910e-03,  2.3003e-02,  2.5892e-02,\n",
            "        -1.9408e-02, -3.7224e-02, -1.3968e-03, -1.0504e-02, -3.8191e-02,\n",
            "        -2.3751e-02, -2.5784e-02, -3.8722e-02,  2.7439e-02, -6.4646e-03,\n",
            "         1.6967e-02,  1.1092e-02,  1.9144e-02, -3.6298e-03, -3.1538e-02,\n",
            "        -2.2497e-02, -1.0022e-02, -2.4051e-02, -1.4457e-02, -6.6460e-03,\n",
            "        -4.5893e-03,  1.8412e-02, -8.6618e-03,  7.1125e-03, -3.0982e-02,\n",
            "         6.3786e-03, -4.1203e-02, -1.6147e-03,  5.3711e-03, -4.1542e-02,\n",
            "         1.5601e-02, -8.8579e-03, -1.2896e-02,  4.9332e-03, -9.1966e-03,\n",
            "         2.6151e-02, -1.2737e-02,  1.2471e-02, -3.0492e-02,  1.9704e-02,\n",
            "        -1.1934e-03, -1.4007e-02,  2.8582e-02,  1.5210e-02, -1.3897e-02,\n",
            "         4.7068e-03, -3.3612e-02, -2.9508e-02, -1.3967e-02,  2.4407e-02,\n",
            "        -2.8429e-02, -1.7446e-02, -1.6473e-03,  1.1476e-02,  2.3820e-02,\n",
            "         9.4632e-03, -1.1367e-02, -2.5504e-02,  2.4133e-03,  1.4580e-02,\n",
            "         6.2194e-03,  1.9586e-02, -2.0713e-02, -1.1198e-02, -2.5153e-02,\n",
            "        -2.9801e-02, -3.5709e-02, -8.8935e-04, -1.4486e-02, -7.8161e-03,\n",
            "         5.3412e-03, -3.9568e-02, -2.2496e-02,  5.0232e-03,  2.4690e-02,\n",
            "        -5.0750e-03, -2.5163e-02,  6.8844e-03, -1.6248e-02, -1.5141e-02,\n",
            "        -2.0825e-02,  2.9837e-02,  2.5213e-02,  2.8178e-02,  1.8569e-02,\n",
            "         5.5204e-03,  7.3526e-03,  1.7414e-02,  1.0863e-02, -3.2228e-02,\n",
            "        -3.5786e-05, -2.7900e-02, -1.3481e-03,  2.4515e-02, -1.8620e-02,\n",
            "         2.0511e-02,  9.0197e-03, -2.1332e-02, -1.3012e-02, -2.0516e-03,\n",
            "        -3.8984e-02, -4.1749e-02,  1.6676e-02,  7.6127e-03,  2.4724e-02,\n",
            "        -3.0855e-02, -5.0847e-04, -2.9218e-02, -9.9025e-03,  2.3272e-02,\n",
            "        -1.4982e-02, -2.4391e-02,  2.2484e-02, -3.8849e-02, -2.9454e-02,\n",
            "        -1.7872e-02, -3.2968e-02, -1.4560e-02, -2.1717e-02, -1.7731e-02,\n",
            "        -1.6575e-02, -2.1482e-02,  2.2850e-03,  1.9191e-02, -4.0660e-02,\n",
            "         2.0548e-02, -2.0533e-02, -1.5806e-02, -4.0070e-03, -1.2059e-02,\n",
            "        -3.8022e-03,  1.2241e-02,  1.7976e-02, -1.6694e-02,  1.1471e-02,\n",
            "        -6.1750e-03, -3.8929e-02]), 'net.3.weight': tensor([[ 0.0113,  0.0442,  0.0291,  ...,  0.0027, -0.0621,  0.0254],\n",
            "        [-0.0053, -0.0115, -0.0363,  ..., -0.0270,  0.0101, -0.0180],\n",
            "        [ 0.0359, -0.0050, -0.0406,  ..., -0.0542, -0.0068, -0.0086],\n",
            "        ...,\n",
            "        [-0.0422,  0.0260, -0.0340,  ...,  0.0214,  0.0145,  0.0213],\n",
            "        [ 0.0268,  0.0348, -0.0225,  ..., -0.0330, -0.0180, -0.0342],\n",
            "        [ 0.0085, -0.0202, -0.0280,  ..., -0.0308, -0.0575, -0.0166]]), 'net.3.bias': tensor([ 0.0254,  0.0466, -0.0329,  0.0191,  0.0384,  0.0222, -0.0474, -0.0563,\n",
            "         0.0300,  0.0107,  0.0487,  0.0144, -0.0105, -0.0273, -0.0639, -0.0035,\n",
            "         0.0214,  0.0234,  0.0251, -0.0205, -0.0452, -0.0458, -0.0142,  0.0186,\n",
            "         0.0121,  0.0385, -0.0105, -0.0214,  0.0251,  0.0098,  0.0585, -0.0237,\n",
            "         0.0212,  0.0042,  0.0113, -0.0252, -0.0560,  0.0033,  0.0106,  0.0189,\n",
            "         0.0041, -0.0170,  0.0635,  0.0301, -0.0166,  0.0251, -0.0304, -0.0230,\n",
            "         0.0273,  0.0434, -0.0269,  0.0477,  0.0379, -0.0180,  0.0227, -0.0350,\n",
            "         0.0391,  0.0142,  0.0370,  0.0119, -0.0314,  0.0267, -0.0363, -0.0463,\n",
            "        -0.0133,  0.0323, -0.0393, -0.0490,  0.0653,  0.0316, -0.0602,  0.0152,\n",
            "         0.0048, -0.0392,  0.0087,  0.0529,  0.0540,  0.0321, -0.0179,  0.0059,\n",
            "         0.0244, -0.0302,  0.0258, -0.0151,  0.0278, -0.0085, -0.0191, -0.0392,\n",
            "         0.0396,  0.0103, -0.0061, -0.0125,  0.0449,  0.0261, -0.0314, -0.0090,\n",
            "        -0.0521, -0.0030, -0.0325,  0.0116, -0.0175, -0.0104, -0.0227, -0.0382,\n",
            "        -0.0205, -0.0177,  0.0163,  0.0226, -0.0237, -0.0357,  0.0240, -0.0343,\n",
            "        -0.0138,  0.0373,  0.0135,  0.0387,  0.0269,  0.0178, -0.0472,  0.0374,\n",
            "         0.0083,  0.0206,  0.0053,  0.0244, -0.0132,  0.0093, -0.0588, -0.0171,\n",
            "        -0.0285,  0.0294, -0.0319, -0.0322,  0.0324,  0.0224,  0.0115,  0.0158,\n",
            "        -0.0144, -0.0159, -0.0024,  0.0373,  0.0087, -0.0058, -0.0083, -0.0345,\n",
            "         0.0212, -0.0415, -0.0444,  0.0288,  0.0037,  0.0274,  0.0165,  0.0472,\n",
            "         0.0333,  0.0226,  0.0222, -0.0211, -0.0009, -0.0247, -0.0544, -0.0430,\n",
            "        -0.0368, -0.0399, -0.0076, -0.0193, -0.0168, -0.0417, -0.0294,  0.0061,\n",
            "         0.0024,  0.0211,  0.0055,  0.0275, -0.0314,  0.0121,  0.0038, -0.0605,\n",
            "        -0.0200, -0.0238, -0.0388,  0.0511, -0.0335, -0.0366,  0.0262, -0.0376,\n",
            "        -0.0154,  0.0405,  0.0106,  0.0177,  0.0049, -0.0009, -0.0091, -0.0366,\n",
            "         0.0016,  0.0146, -0.0417,  0.0587, -0.0236, -0.0087,  0.0198, -0.0305,\n",
            "        -0.0235, -0.0532,  0.0337,  0.0019,  0.0136, -0.0249,  0.0181, -0.0499,\n",
            "         0.0346, -0.0383,  0.0060,  0.0445,  0.0086, -0.0249,  0.0016, -0.0329,\n",
            "         0.0522, -0.0515, -0.0372, -0.0291, -0.0494,  0.0423, -0.0416, -0.0222,\n",
            "         0.0078,  0.0359,  0.0054, -0.0521,  0.0584,  0.0416, -0.0308,  0.0465,\n",
            "         0.0208, -0.0361,  0.0007,  0.0274,  0.0171,  0.0105,  0.0335, -0.0391,\n",
            "         0.0149,  0.0169,  0.0391, -0.0365, -0.0023, -0.0223, -0.0067,  0.0136,\n",
            "         0.0335, -0.0098, -0.0504,  0.0016,  0.0403, -0.0503,  0.0077, -0.0461]), 'net.5.weight': tensor([[-0.0488, -0.0449, -0.0795,  ...,  0.0178, -0.0084, -0.0139],\n",
            "        [-0.0242, -0.0736,  0.0106,  ...,  0.0277,  0.0450, -0.0168],\n",
            "        [ 0.0440, -0.0444, -0.0515,  ..., -0.0405, -0.0432,  0.0197],\n",
            "        ...,\n",
            "        [ 0.0002,  0.0176, -0.0812,  ..., -0.0344, -0.0199, -0.0420],\n",
            "        [ 0.0129,  0.0234,  0.0592,  ...,  0.0699, -0.0391, -0.0088],\n",
            "        [ 0.0165, -0.0335,  0.0638,  ..., -0.0175,  0.0443,  0.0089]]), 'net.5.bias': tensor([ 0.0419,  0.0144,  0.0276,  0.0114,  0.0616,  0.0386, -0.0383,  0.0316,\n",
            "         0.0445,  0.0513])})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StudentMLP(nn.Module):\n",
        "  def __init__(self, hidden=128):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(28*28, hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden,10)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "4yoUNCSEWu-M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student = StudentMLP(hidden=128)"
      ],
      "metadata": {
        "id": "J7sxzpW6WvAq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(student)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM2WRnoJWvDU",
        "outputId": "e540739e-5715-4a61-cf0a-64000a4696ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StudentMLP(\n",
            "  (net): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_student(student, loader, epochs=1, lr=1e-3):\n",
        "  student.train()\n",
        "  opt = optim.Adam(student.parameters(), lr=lr)\n",
        "  ce_loss = nn.CrossEntropyLoss()\n",
        "  for ep in range(epochs):\n",
        "    for x,y in loader:\n",
        "      opt.zero_grad()\n",
        "      out = student(x)\n",
        "      loss = ce_loss(out,y)\n",
        "      loss.backward()\n",
        "      opt.step()"
      ],
      "metadata": {
        "id": "TmBacxqwitAI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrain_student(student, train_loader, epochs=1)"
      ],
      "metadata": {
        "id": "CCpMmvSNjHoA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the trained student model, so what we will do is we will take the soft labels from the teacher model and then use the hard labels of the student model."
      ],
      "metadata": {
        "id": "xqNj9_Yzdz9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distil Training"
      ],
      "metadata": {
        "id": "9E98PqpjeM6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 2.0\n",
        "alpha = 0.7\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "optimizer = optim.Adam(student.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "5DZBv4wwWvFp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distil(student, teacher, loader, epochs=1):\n",
        "  for ep in range(epochs):\n",
        "\n",
        "    student.train() # This will not start the model training, this will just initialise the training.\n",
        "\n",
        "    total_loss = 0 # We will calculate this later\n",
        "\n",
        "    # Teacher model outputs\n",
        "    for x,y in loader: # Here x is the 1d array and y is the ground truth\n",
        "      with torch.no_grad():  # Torch.no_grad means that we are not going to update the teacher models gradients, this means we are not going to train the model.\n",
        "        t_logits = teacher(x)\n",
        "        t_probs = torch.softmax(t_logits / temperature, dim=1)\n",
        "\n",
        "    # Student model outputs\n",
        "      s_logits = student(x)\n",
        "      s_log_probs = torch.log_softmax(s_logits / temperature, dim=1)\n",
        "\n",
        "      # Calculating the Losses\n",
        "      loss_soft = kl_loss(s_log_probs, t_probs) * (temperature ** 2)\n",
        "      loss_hard = ce_loss(s_logits, y)\n",
        "      loss = alpha * loss_soft + (1 - alpha) * loss_hard\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "    print(f\" Student Epoch {ep} Loss {total_loss/len(loader):.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GcZwQ6lnWvIO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distil(student,teacher,train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEkL5h1nWvK1",
        "outputId": "c6821922-9552-4545-8d98-ba070e128695"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Student Epoch 0 Loss 0.1454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the performances of the teacher and student models"
      ],
      "metadata": {
        "id": "gHlREhawgp9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, name=\"Model\"):\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      out = model(x)\n",
        "      preds = out.argmax(dim=1)\n",
        "      correct += (preds == y).sum().item()\n",
        "      total += y.size(0)\n",
        "  acc = correct / total * 100\n",
        "\n",
        "  print(f\"{name} Accuracy: {acc:.2f}%\")\n",
        "  return acc"
      ],
      "metadata": {
        "id": "BhYAu9uQWvNS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(teacher, test_loader, \"Teacher\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBu8VWDgWvQC",
        "outputId": "3120330c-bb61-4855-d23a-49022ec7e79e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Accuracy: 96.08%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.08"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(student, test_loader, \"Student\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQFR0vFhhGLW",
        "outputId": "7a671835-98dc-40de-d62e-0e5dd093944f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Accuracy: 95.73%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.73"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that if we use the pretrained student model, then the accuracy of the model is even higher and the model being smaller is very much usable in day to day tasks."
      ],
      "metadata": {
        "id": "uYgNqRIOhKkY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-teaJohhtdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZs5Px5uhte4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x0pAXULRhtg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vpP6QAdjhtiI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}