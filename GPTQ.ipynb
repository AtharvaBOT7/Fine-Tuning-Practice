{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11431c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b45f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
      "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/Context.cpp:85.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BACKEND',\n",
       " 'BaseQuantizeConfig',\n",
       " 'GPTQModel',\n",
       " 'QuantizeConfig',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'adapter',\n",
       " 'exllama_set_max_input_length',\n",
       " 'get_best_device',\n",
       " 'looper',\n",
       " 'models',\n",
       " 'nn_modules',\n",
       " 'os',\n",
       " 'quantization',\n",
       " 'utils',\n",
       " 'version']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gptqmodel\n",
    "dir(gptqmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e33b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptqmodel import GPTQModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2a8eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_quantized: adapter: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 46798.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (MPS or XPU): `torch.float16`                         \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.85 bpw, based on [bits: 4, group_size: 32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `FORMAT.GPTQ` to internal `FORMAT.GPTQ_V2`.\n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                         \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.008443832397460938s                       \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=128004 (token='<|finetune_right_pad_id|>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Model: `generation_config.json` not found. Skipped checking.             \n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                   \n"
     ]
    }
   ],
   "source": [
    "model = GPTQModel.load(\"ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d212eb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaGPTQ(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): TorchQuantLinear()\n",
       "            (k_proj): TorchQuantLinear()\n",
       "            (v_proj): TorchQuantLinear()\n",
       "            (o_proj): TorchQuantLinear()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): TorchQuantLinear()\n",
       "            (up_proj): TorchQuantLinear()\n",
       "            (down_proj): TorchQuantLinear()\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8869a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.generate(\"Explain the theory of relativity in simple terms.\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857e119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Explain the theory of relativity in simple terms. Albert Einstein's theory of relativity revolutionized our understanding of space and time.\n",
      "\n",
      "## Step 1\n"
     ]
    }
   ],
   "source": [
    "print(model.tokenizer.decode(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d5e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244c24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9187b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_path = \"Llama-3.2-1B-Instruct-gptqmodel-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bcaf213",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_dataset = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    data_files = \"en/c4-train.00001-of-01024.json.gz\",\n",
    "    split=\"train\",\n",
    ").select(range(1024))[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb97981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptqmodel import GPTQModel, QuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a357bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = QuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa484a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe55538",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff0755ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=HUGGINGFACE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d26ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edaba1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-24T01:16:12.446414Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mReqwest(reqwest::Error { kind: Request, url: \"https://transfer.xethub.hf.co/xorbs/default/fc70bc7aaaca80289f25cc565996093c1b91929b74b45b8be4b98666b60b5a82?X-Xet-Signed-Range=bytes%3D56999338-57984345&X-Xet-Session-Id=01K89WJ7JWAQQ177G2C9RS7YYY&Expires=1761272141&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly90cmFuc2Zlci54ZXRodWIuaGYuY28veG9yYnMvZGVmYXVsdC9mYzcwYmM3YWFhY2E4MDI4OWYyNWNjNTY1OTk2MDkzYzFiOTE5MjliNzRiNDViOGJlNGI5ODY2NmI2MGI1YTgyP1gtWGV0LVNpZ25lZC1SYW5nZT1ieXRlcyUzRDU2OTk5MzM4LTU3OTg0MzQ1JlgtWGV0LVNlc3Npb24tSWQ9MDFLODlXSjdKV0FRUTE3N0cyQzlSUzdZWVkiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjEyNzIxNDF9fX1dfQ__&Signature=tUdAqAvNbPAfhEemMga4L~de-kzMofiqGTedyLVJ8jEh5wuZZLQn3EiGXZwBYDcqcixFVTqtAHkYxbTe5Rko4qgby7PGkIor9nA6FIZxwX83QlLLPJBrv4s7C51eH-GXiLp1PSaOhnqFjpG3nw8dApDxtB2YMEK72Ap8fXEtmjgdkwZIqbiTMeInHsyVffi3ty1ZJGHHF-RoINa0bqyFsaQ06pnUuGSg9oqdsjLEYpFNrWHUSezM6aY1mXmkRB39SRYentGSPmAv4Ibav2K2yUwyawrbYYAhTYlk85KttnmWW30NWp5Xod25K7xfFx~GI7S6seTBHqFqK3Qne94ENQ__&Key-Pair-Id=K2L8F4GPSG1IFC\", source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(IncompleteMessage)) }). Retrying...\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:233\n",
      "\n",
      "  \u001b[2m2025-10-24T01:16:12.446475Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #0. Sleeping 1.602038657s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "  \u001b[2m2025-10-24T01:16:12.450008Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mReqwest(reqwest::Error { kind: Request, url: \"https://transfer.xethub.hf.co/xorbs/default/44002026dc3e94503a39c76fb31eff9e6b60e13ad523daf488b1eca01c947565?X-Xet-Signed-Range=bytes%3D9790330-16968146&X-Xet-Session-Id=01K89WJ7JWAQQ177G2C9RS7YYY&Expires=1761272141&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly90cmFuc2Zlci54ZXRodWIuaGYuY28veG9yYnMvZGVmYXVsdC80NDAwMjAyNmRjM2U5NDUwM2EzOWM3NmZiMzFlZmY5ZTZiNjBlMTNhZDUyM2RhZjQ4OGIxZWNhMDFjOTQ3NTY1P1gtWGV0LVNpZ25lZC1SYW5nZT1ieXRlcyUzRDk3OTAzMzAtMTY5NjgxNDYmWC1YZXQtU2Vzc2lvbi1JZD0wMUs4OVdKN0pXQVFRMTc3RzJDOVJTN1lZWSIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTI3MjE0MX19fV19&Signature=XaZA9cM5KFPaod2Eq1bwcL6-DHhfEDVUt1QE2LliYMkdAY9yTehIS-dqZclYBz8g2GDJhFSsQRrjqOkuT8D1nM0v9cXAlack9qjvnHRsDCU9hFj0y3f861YWtD2a6GQWafl9w3UCZzV7PQMKHsmJajgnOAl9jrsH~n7Uo0FRkeiSIAKao8gZ131stq2jupn-moEzIlM2g5d4T9O26qySYZmTkWGl8XL0t1uWRhLg2WLvJYmfC41kG0fw05dv-xx~B0m4VCcV51V0Gq80f-ICphbVNbAtFL2y9-OBXZzwcaQehKdvhZ9Sst-8WZYIZLRvaYd-EsOSn92dc1415iDhVA__&Key-Pair-Id=K2L8F4GPSG1IFC\", source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(IncompleteMessage)) }). Retrying...\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:233\n",
      "\n",
      "  \u001b[2m2025-10-24T01:16:12.450044Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #0. Sleeping 2.623412332s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [01:39<00:00,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (MPS or XPU): `torch.float16`                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=128004 (token='<|finetune_right_pad_id|>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   \n"
     ]
    }
   ],
   "source": [
    "model = GPTQModel.load(model_id,quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a6e8cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaGPTQ(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): HookedLinear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): HookedLinear(in_features=2048, out_features=512, bias=False)\n",
       "            (v_proj): HookedLinear(in_features=2048, out_features=512, bias=False)\n",
       "            (o_proj): HookedLinear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): HookedLinear(in_features=2048, out_features=8192, bias=False)\n",
       "            (up_proj): HookedLinear(in_features=2048, out_features=8192, bias=False)\n",
       "            (down_proj): HookedLinear(in_features=8192, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, torch\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# keep model on CPU for quantize()\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c698a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[32mINFO\u001b[0m  Hooked Modules: Using tree based config for accurate targeting of modules\n",
      "Quantizing self_attn.k_proj in layer  [0 of 15] | 0:00:52 / 0:13:52 [1/16] 6.2%"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::cholesky_inverse' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 0fabc3ba44823f257e70ce397d989c8de5e362c1. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalibration_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/gptqmodel/models/base.py:493\u001b[39m, in \u001b[36mBaseGPTQModel.quantize\u001b[39m\u001b[34m(self, calibration_dataset, calibration_dataset_concat_size, batch_size, calibration_enable_gpu_cache, tokenizer, logger_board, backend, buffered_fwd, auto_gc, adapter, adapter_calibration_dataset, calibration_data_min_length, fail_safe)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# prepare processor worker (looper)\u001b[39;00m\n\u001b[32m    491\u001b[39m module_looper = ModuleLooper(\u001b[38;5;28mself\u001b[39m, processors=processors)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule_looper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcalibration_enable_gpu_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcalibration_enable_gpu_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffered_fwd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffered_fwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_gc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_gc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfail_safe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfail_safe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/gptqmodel/looper/module_looper.py:419\u001b[39m, in \u001b[36mModuleLooper.loop\u001b[39m\u001b[34m(self, auto_gc, calibration_enable_gpu_cache, buffered_fwd, fail_safe, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name_index, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(subset):\n\u001b[32m    418\u001b[39m         m = subset[name]\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m         \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_gc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_gc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m         processed_subset[name] = m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    422\u001b[39m     \u001b[38;5;66;03m# TODO: there are threading/sync issues with streaming transfers\u001b[39;00m\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# for name in subset:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    428\u001b[39m \n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# set to number of devices\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/gptqmodel/looper/gptq_processor.py:144\u001b[39m, in \u001b[36mGPTQProcessor.process\u001b[39m\u001b[34m(self, module, auto_gc)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lock:\n\u001b[32m    142\u001b[39m     g = \u001b[38;5;28mself\u001b[39m.tasks[module.name]\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m wq, scale, zero, g_idx, duration, avg_loss, damp_percent, nsamples = \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lock:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mself\u001b[39m.result_save(module.full_name, {\n\u001b[32m    148\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m: scale,\n\u001b[32m    149\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzero\u001b[39m\u001b[33m\"\u001b[39m: zero,\n\u001b[32m    150\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mg_idx\u001b[39m\u001b[33m\"\u001b[39m: g_idx,\n\u001b[32m    151\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/gptqmodel/quantization/gptq.py:411\u001b[39m, in \u001b[36mGPTQ.quantize\u001b[39m\u001b[34m(self, blocksize)\u001b[39m\n\u001b[32m    408\u001b[39m Losses = torch.zeros_like(W)\n\u001b[32m    409\u001b[39m Q = torch.zeros_like(W)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m Hinv, damp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhessian_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# Use simplified loop when mock_quantization is active\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.qcfg.mock_quantization \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.fail_safe \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fwd_counter == \u001b[32m0\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/gptq/lib/python3.11/site-packages/gptqmodel/quantization/gptq.py:305\u001b[39m, in \u001b[36mGPTQ.hessian_inverse\u001b[39m\u001b[34m(self, H)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;66;03m# TODO call to torch.linalg is not threadsafe? Porque no? Esta muy mal.\u001b[39;00m\n\u001b[32m    304\u001b[39m H2 = torch.linalg.cholesky(H2)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m Hinv = torch.linalg.cholesky(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH2\u001b[49m\u001b[43m)\u001b[49m, upper=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m H, H2\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: The operator 'aten::cholesky_inverse' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 0fabc3ba44823f257e70ce397d989c8de5e362c1. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "model.quantize(calibration_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f97f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180754d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
