{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/atharvabot7/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/check/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "W0927 20:42:51.648000 32228 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
            "/Users/atharvabot7/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/check/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/Users/atharvabot7/Downloads/MLOPS-Tutorials/Fine-Tuning-Practice/check/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'train'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 80\u001b[0m\n\u001b[1;32m     68\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     69\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     70\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtok,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     formatting_func\u001b[38;5;241m=\u001b[39mformatting_func,\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trainer\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: trainer\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[1;32m     82\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     83\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(new_model)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'train'"
          ]
        }
      ],
      "source": [
        "# ---- LoRA fine-tuning on Mac (MPS), no bitsandbytes ----\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, pipeline\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "output_dir = \"./lora_out\"\n",
        "new_model = \"tinyllama-lora-adapter\"\n",
        "\n",
        "def fmt(row):\n",
        "    if \"text\" in row:\n",
        "        return {\"text\": row[\"text\"]}\n",
        "    if all(k in row for k in [\"instruction\", \"output\"]):\n",
        "        user = row[\"instruction\"]\n",
        "        if \"input\" in row and row[\"input\"]:\n",
        "            user += f\"\\n{row['input']}\"\n",
        "        return {\"text\": f\"<|system|>You are helpful.<|user|>{user}<|assistant|>{row['output']}\"}\n",
        "    if all(k in row for k in [\"question\", \"answer\"]):\n",
        "        return {\"text\": f\"<|user|>{row['question']}<|assistant|>{row['answer']}\"}\n",
        "    if all(k in row for k in [\"prompt\", \"response\"]):\n",
        "        return {\"text\": f\"<|user|>{row['prompt']}<|assistant|>{row['response']}\"}\n",
        "    raise KeyError(f\"Don't know how to format row with keys: {list(row.keys())}\")\n",
        "\n",
        "ds = load_dataset(dataset_name, split=\"train\")\n",
        "ds = ds.map(fmt, remove_columns=ds.column_names)\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "if tok.pad_token_id is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "tok.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.config.use_cache = False\n",
        "\n",
        "peft_cfg = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        ")\n",
        "\n",
        "# NOTE: removed evaluation_strategy (older transformers doesn’t accept it)\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.0,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    report_to=\"none\",\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    dataloader_num_workers=0,\n",
        "    dataloader_pin_memory=False,\n",
        "    optim=\"adamw_torch\",\n",
        ")\n",
        "\n",
        "def formatting_func(batch):\n",
        "    return batch[\"text\"]\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tok,\n",
        "    train_dataset=ds,\n",
        "    args=args,\n",
        "    peft_config=peft_cfg,\n",
        "    max_seq_length=1024,\n",
        "    packing=False,\n",
        "    formatting_func=formatting_func,\n",
        ")\n",
        "\n",
        "if not hasattr(trainer.optimizer, \"train\"):\n",
        "    trainer.optimizer.train = lambda *args, **kwargs: trainer.optimizer\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(new_model)\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=trainer.model, tokenizer=tok, max_new_tokens=128)\n",
        "print(pipe(\"<|user|>What is a large language model?<|assistant|>\")[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9.6\n"
          ]
        }
      ],
      "source": [
        "import trl; print(trl.__version__)  # expect 0.11.x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ---- LoRA fine-tuning on Mac (MPS), no bitsandbytes ----\n",
        "# import torch\n",
        "# from datasets import load_dataset\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, pipeline\n",
        "# from peft import LoraConfig\n",
        "# from trl import SFTTrainer\n",
        "\n",
        "# # 0) pick a SMALL chat model for 18GB RAM; 1B–3B is safe\n",
        "# # (If you have access and enough memory, you can try Llama-3.2-3B-Instruct)\n",
        "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# dataset_name = \"mlabonne/guanaco-llama2-1k\"   # example instruction dataset\n",
        "# output_dir = \"./lora_out\"\n",
        "# new_model = \"tinyllama-lora-adapter\"\n",
        "\n",
        "# # 1) dataset → unify to single 'text' field (no multiprocessing on macOS notebooks)\n",
        "# def fmt(row):\n",
        "#     # case 1: already has 'text'\n",
        "#     if \"text\" in row:\n",
        "#         return {\"text\": row[\"text\"]}\n",
        "#     # case 2: Alpaca-style (instruction + input + output)\n",
        "#     if all(k in row for k in [\"instruction\", \"output\"]):\n",
        "#         user = row[\"instruction\"]\n",
        "#         if \"input\" in row and row[\"input\"]:\n",
        "#             user += f\"\\n{row['input']}\"\n",
        "#         return {\"text\": f\"<|system|>You are helpful.<|user|>{user}<|assistant|>{row['output']}\"}\n",
        "#     # case 3: Q/A style\n",
        "#     if all(k in row for k in [\"question\", \"answer\"]):\n",
        "#         return {\"text\": f\"<|user|>{row['question']}<|assistant|>{row['answer']}\"}\n",
        "#     # case 4: prompt/response style\n",
        "#     if all(k in row for k in [\"prompt\", \"response\"]):\n",
        "#         return {\"text\": f\"<|user|>{row['prompt']}<|assistant|>{row['response']}\"}\n",
        "#     # fallback\n",
        "#     raise KeyError(f\"Don't know how to format row with keys: {list(row.keys())}\")\n",
        "\n",
        "# ds = load_dataset(dataset_name, split=\"train\")\n",
        "# print(\"Dataset columns:\", ds.column_names)  # sanity check\n",
        "# ds = ds.map(fmt, remove_columns=ds.column_names)\n",
        "\n",
        "# # 2) tokenizer & base model (no .to(\"mps\"); Trainer handles device)\n",
        "# tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "# if tok.pad_token_id is None:\n",
        "#     tok.pad_token = tok.eos_token\n",
        "# tok.padding_side = \"right\"\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "# model.config.use_cache = False  # avoid warning with grad checkpointing\n",
        "\n",
        "# # 3) LoRA config (targets for LLaMA/Mistral/TinyLlama)\n",
        "# peft_cfg = LoraConfig(\n",
        "#     r=8,\n",
        "#     lora_alpha=16,\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "# )\n",
        "\n",
        "# # 4) training args tuned for MPS (Apple Silicon)\n",
        "# args = TrainingArguments(\n",
        "#     output_dir=output_dir,\n",
        "#     num_train_epochs=1,\n",
        "#     per_device_train_batch_size=2,\n",
        "#     per_device_eval_batch_size=2,\n",
        "#     gradient_accumulation_steps=8,     # effective batch 16\n",
        "#     learning_rate=2e-4,\n",
        "#     warmup_ratio=0.03,\n",
        "#     weight_decay=0.0,\n",
        "#     lr_scheduler_type=\"cosine\",\n",
        "#     logging_steps=20,\n",
        "#     save_steps=200,\n",
        "#     evaluation_strategy=\"no\",\n",
        "#     report_to=\"none\",\n",
        "#     fp16=False,                        # not on MPS\n",
        "#     bf16=True,                         # ✅ M-series friendly\n",
        "#     dataloader_num_workers=0,          # macOS stability\n",
        "#     dataloader_pin_memory=False,       # MPS quirk\n",
        "# )\n",
        "\n",
        "# # 5) SFT trainer (let TRL apply LoRA via peft_config)\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model,\n",
        "#     tokenizer=tok,\n",
        "#     train_dataset=ds,\n",
        "#     args=args,\n",
        "#     peft_config=peft_cfg,\n",
        "#     max_seq_length=1024,\n",
        "#     packing=False,\n",
        "#     formatting_func=lambda batch: [ex[\"text\"] for ex in batch],\n",
        "# )\n",
        "\n",
        "# trainer.train()\n",
        "# trainer.model.save_pretrained(new_model)  # saves LoRA adapters\n",
        "\n",
        "# # 6) quick test generation with the fine-tuned model (adapters already attached)\n",
        "# pipe = pipeline(\"text-generation\", model=trainer.model, tokenizer=tok, max_new_tokens=128)\n",
        "# print(pipe(\"<|user|>What is a large language model?<|assistant|>\")[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "check",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
